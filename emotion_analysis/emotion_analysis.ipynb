{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def rm_tags(text):\n",
    "    re_tag = re.compile(r'<[^>]+>')\n",
    "    return re_tag.sub('', text) #用空字符替换"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv1D, MaxPooling1D, Dropout, Input, concatenate\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.models import Model\n",
    "import os\n",
    "import tarfile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"    \n",
    "# import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#构造函数读取train和test中的所有影评\n",
    "def read_files(filetype):\n",
    "    \"\"\"\n",
    "    filetype: 'train' or 'test'\n",
    "    return type: \n",
    "                all_texts:filetype数据文本\n",
    "                all_labels:filetype数据标签\n",
    "    \"\"\"\n",
    "    all_labels = [1]*12500 + [0]*12500\n",
    "    all_texts = []\n",
    "    file_list = []\n",
    "    path = r'./aclImdb/'  #r 表示./aclImdb/是raw string 不需要转义\n",
    "    #读取pos文本名\n",
    "    pos_path = path + filetype + '/pos/'\n",
    "    for file in os.listdir(pos_path):\n",
    "        file_list.append(pos_path+file)\n",
    "    #读取neg文本名\n",
    "    neg_path = path + filetype + '/neg/'\n",
    "    for file in os.listdir(neg_path):\n",
    "        file_list.append(neg_path+file)\n",
    "    print(file_list[0])\n",
    "    #将pos文本和neg文本添加到all_texts\n",
    "    for file_name in file_list:\n",
    "        with open(file_name, encoding = 'utf-8') as f:\n",
    "            all_texts.append(rm_tags(\"\".join(f.readlines())))\n",
    "    return all_texts, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "seq = (\"a\", \"b\", \"c\") # 字符串序列\n",
    "print (\"\".join(seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./aclImdb/train/pos/0_9.txt\n"
     ]
    }
   ],
   "source": [
    "#将评论数据和标签分别载入\n",
    "x_train,y_train  = read_files('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High\\'s satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers\\' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I\\'m here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn\\'t!'"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#用keras进行分词\n",
    "for cut in range(len(x_train)):\n",
    "     x_train[cut]= keras.preprocessing.text.text_to_word_sequence(x_train[cut],\n",
    "                                               filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n',\n",
    "                                               lower=True,\n",
    "                                               split=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'a',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'it',\n",
       " 'ran',\n",
       " 'at',\n",
       " 'the',\n",
       " 'same',\n",
       " 'time',\n",
       " 'as',\n",
       " 'some',\n",
       " 'other',\n",
       " 'programs',\n",
       " 'about',\n",
       " 'school',\n",
       " 'life',\n",
       " 'such',\n",
       " 'as',\n",
       " 'teachers',\n",
       " 'my',\n",
       " '35',\n",
       " 'years',\n",
       " 'in',\n",
       " 'the',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'me',\n",
       " 'to',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'bromwell',\n",
       " \"high's\",\n",
       " 'satire',\n",
       " 'is',\n",
       " 'much',\n",
       " 'closer',\n",
       " 'to',\n",
       " 'reality',\n",
       " 'than',\n",
       " 'is',\n",
       " 'teachers',\n",
       " 'the',\n",
       " 'scramble',\n",
       " 'to',\n",
       " 'survive',\n",
       " 'financially',\n",
       " 'the',\n",
       " 'insightful',\n",
       " 'students',\n",
       " 'who',\n",
       " 'can',\n",
       " 'see',\n",
       " 'right',\n",
       " 'through',\n",
       " 'their',\n",
       " 'pathetic',\n",
       " \"teachers'\",\n",
       " 'pomp',\n",
       " 'the',\n",
       " 'pettiness',\n",
       " 'of',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'situation',\n",
       " 'all',\n",
       " 'remind',\n",
       " 'me',\n",
       " 'of',\n",
       " 'the',\n",
       " 'schools',\n",
       " 'i',\n",
       " 'knew',\n",
       " 'and',\n",
       " 'their',\n",
       " 'students',\n",
       " 'when',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'the',\n",
       " 'episode',\n",
       " 'in',\n",
       " 'which',\n",
       " 'a',\n",
       " 'student',\n",
       " 'repeatedly',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'burn',\n",
       " 'down',\n",
       " 'the',\n",
       " 'school',\n",
       " 'i',\n",
       " 'immediately',\n",
       " 'recalled',\n",
       " 'at',\n",
       " 'high',\n",
       " 'a',\n",
       " 'classic',\n",
       " 'line',\n",
       " 'inspector',\n",
       " \"i'm\",\n",
       " 'here',\n",
       " 'to',\n",
       " 'sack',\n",
       " 'one',\n",
       " 'of',\n",
       " 'your',\n",
       " 'teachers',\n",
       " 'student',\n",
       " 'welcome',\n",
       " 'to',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 'i',\n",
       " 'expect',\n",
       " 'that',\n",
       " 'many',\n",
       " 'adults',\n",
       " 'of',\n",
       " 'my',\n",
       " 'age',\n",
       " 'think',\n",
       " 'that',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 'is',\n",
       " 'far',\n",
       " 'fetched',\n",
       " 'what',\n",
       " 'a',\n",
       " 'pity',\n",
       " 'that',\n",
       " 'it',\n",
       " \"isn't\"]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#去掉每条评论中的停用词\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stopwords = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "179"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(english_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(x_train)):\n",
    "    x_train[i] = [word for word in x_train[i] \n",
    "                  if not word in english_stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['bromwell',\n",
       " 'high',\n",
       " 'cartoon',\n",
       " 'comedy',\n",
       " 'ran',\n",
       " 'time',\n",
       " 'programs',\n",
       " 'school',\n",
       " 'life',\n",
       " 'teachers',\n",
       " '35',\n",
       " 'years',\n",
       " 'teaching',\n",
       " 'profession',\n",
       " 'lead',\n",
       " 'believe',\n",
       " 'bromwell',\n",
       " \"high's\",\n",
       " 'satire',\n",
       " 'much',\n",
       " 'closer',\n",
       " 'reality',\n",
       " 'teachers',\n",
       " 'scramble',\n",
       " 'survive',\n",
       " 'financially',\n",
       " 'insightful',\n",
       " 'students',\n",
       " 'see',\n",
       " 'right',\n",
       " 'pathetic',\n",
       " \"teachers'\",\n",
       " 'pomp',\n",
       " 'pettiness',\n",
       " 'whole',\n",
       " 'situation',\n",
       " 'remind',\n",
       " 'schools',\n",
       " 'knew',\n",
       " 'students',\n",
       " 'saw',\n",
       " 'episode',\n",
       " 'student',\n",
       " 'repeatedly',\n",
       " 'tried',\n",
       " 'burn',\n",
       " 'school',\n",
       " 'immediately',\n",
       " 'recalled',\n",
       " 'high',\n",
       " 'classic',\n",
       " 'line',\n",
       " 'inspector',\n",
       " \"i'm\",\n",
       " 'sack',\n",
       " 'one',\n",
       " 'teachers',\n",
       " 'student',\n",
       " 'welcome',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 'expect',\n",
       " 'many',\n",
       " 'adults',\n",
       " 'age',\n",
       " 'think',\n",
       " 'bromwell',\n",
       " 'high',\n",
       " 'far',\n",
       " 'fetched',\n",
       " 'pity']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "#tokenizer = Tokenizer(num_words=2000)  # 建立一个2000个单词的字典\n",
    "#tokenizer.fit_on_texts(texts_filtered_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bromwell': 1, 'high': 2, 'teachers': 3, 'school': 4, 'students': 5, 'student': 6, 'cartoon': 7, 'comedy': 8, 'ran': 9, 'time': 10, 'programs': 11, 'life': 12, '35': 13, 'years': 14, 'teaching': 15, 'profession': 16, 'lead': 17, 'believe': 18, \"high's\": 19, 'satire': 20, 'much': 21, 'closer': 22, 'reality': 23, 'scramble': 24, 'survive': 25, 'financially': 26, 'insightful': 27, 'see': 28, 'right': 29, 'pathetic': 30, \"teachers'\": 31, 'pomp': 32, 'pettiness': 33, 'whole': 34, 'situation': 35, 'remind': 36, 'schools': 37, 'knew': 38, 'saw': 39, 'episode': 40, 'repeatedly': 41, 'tried': 42, 'burn': 43, 'immediately': 44, 'recalled': 45, 'classic': 46, 'line': 47, 'inspector': 48, \"i'm\": 49, 'sack': 50, 'one': 51, 'welcome': 52, 'expect': 53, 'many': 54, 'adults': 55, 'age': 56, 'think': 57, 'far': 58, 'fetched': 59, 'pity': 60}\n"
     ]
    }
   ],
   "source": [
    "#print(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#x_train = tokenizer.texts_to_sequences(texts_filtered_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "x_train_seq= {}\n",
    "for index,word in enumerate(texts_filtered_stopwords):\n",
    "    x_train_seq[index+1] = word\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\anaconda\\envs\\ml\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#载入google_news预训练词向量\n",
    "en_model = gensim.models.KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model['China'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "930"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.vocab['student'].index  #每个词在google_news中都有一个index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 获得前100k个词的向量\n",
    "num_words = 100000\n",
    "embedding_matrix = np.zeros((num_words, 300))\n",
    "\n",
    "for i in range(num_words):\n",
    "    embedding_matrix[i,:] = en_model[en_model.index2word[i]]\n",
    "embedding_matrix = embedding_matrix.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'</s>'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_model.index2word[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100000, 300)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for text in x_train:\n",
    "    for i, word in enumerate(text):\n",
    "        try:\n",
    "            # 将词转换为索引index\n",
    "            text[i] = en_model.vocab[word].index\n",
    "        except KeyError :\n",
    "            # 如果词不在字典中，则输出0\n",
    "            text[i] = 0\n",
    "#train_tokens.append(cut_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3861,\n",
       " 4364,\n",
       " 112,\n",
       " 11843,\n",
       " 598,\n",
       " 10893,\n",
       " 314385,\n",
       " 0,\n",
       " 22101,\n",
       " 1289,\n",
       " 1311,\n",
       " 154306,\n",
       " 2738,\n",
       " 11660,\n",
       " 3691,\n",
       " 16205,\n",
       " 11536,\n",
       " 1922,\n",
       " 136460,\n",
       " 1377413,\n",
       " 582,\n",
       " 1852,\n",
       " 65,\n",
       " 95774,\n",
       " 359,\n",
       " 41228,\n",
       " 1279,\n",
       " 14059,\n",
       " 20931,\n",
       " 537477,\n",
       " 0,\n",
       " 5024,\n",
       " 15652,\n",
       " 54,\n",
       " 81,\n",
       " 1067,\n",
       " 11536,\n",
       " 196,\n",
       " 54,\n",
       " 4313,\n",
       " 441,\n",
       " 1161,\n",
       " 13217,\n",
       " 3367,\n",
       " 3213,\n",
       " 1504,\n",
       " 0,\n",
       " 0,\n",
       " 3407,\n",
       " 11997,\n",
       " 8751,\n",
       " 3173,\n",
       " 3781,\n",
       " 112,\n",
       " 155,\n",
       " 459,\n",
       " 213,\n",
       " 56,\n",
       " 1121,\n",
       " 54,\n",
       " 2485,\n",
       " 941,\n",
       " 598,\n",
       " 16919,\n",
       " 2532,\n",
       " 2376,\n",
       " 0,\n",
       " 891,\n",
       " 127,\n",
       " 1021,\n",
       " 1980,\n",
       " 12675,\n",
       " 1120,\n",
       " 897,\n",
       " 692,\n",
       " 548,\n",
       " 2118,\n",
       " 0,\n",
       " 18141,\n",
       " 19527,\n",
       " 17552,\n",
       " 70159,\n",
       " 0,\n",
       " 1939,\n",
       " 401,\n",
       " 0,\n",
       " 1885059,\n",
       " 7293,\n",
       " 251,\n",
       " 1377413,\n",
       " 8807,\n",
       " 1922,\n",
       " 621,\n",
       " 32608,\n",
       " 326,\n",
       " 91,\n",
       " 9136,\n",
       " 370]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[22]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#让每条影片长度相同,假设固定为150，将超出的部分截掉,如果句子小于150，则用0在前面填充\n",
    "x_train = sequence.pad_sequences(x_train,maxlen = 150)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([      0,       0,    5312,  359502,  126464,   13456,    2091,\n",
       "         21264,    1200,    2242,     251,     838,     157,    5173,\n",
       "           109,    4492,  100692,    2318,       0,       0,     158,\n",
       "           588,    1852,   11138,     255,     294,   34073,   21264,\n",
       "         14506,     757,     352,     455,     321,    1995,       0,\n",
       "         21264,    3480,    1482,   23843,    2373,    3342,     274,\n",
       "           408,   72120,     871,    9858,  342999,     482,    9919,\n",
       "             0,  135037,    1513,   17911,   21264,    3346,    3304,\n",
       "           143,     641,     521,  452539,       0,  176638,   95151,\n",
       "          3116,   12825,     193,    5783,    1429,     124,   18498,\n",
       "         18654,  368167,  407864,   23597,  136679,  321442,     311,\n",
       "           233,    1852,   76476,    5320,   21264,  342999,     233,\n",
       "          2770,    7421,    3592,      87,    2242,     821,    1809,\n",
       "          2176,     496,     746,    2008,  452539,   21264,    1343,\n",
       "          4123,     231,     258,   33355,      45,  359502,       0,\n",
       "         48915,    2485,    1467,    3912,     807,    6760,     412,\n",
       "           672,   50177,     141,   20914,   61478,     533, 1276604,\n",
       "             0,     793,     261,      87,     347,    3270,    1429,\n",
       "           147,     113,     660,     321,    6762,    4492,      87,\n",
       "          2242,      68,     177,     237,    1864,     331,    4695,\n",
       "          1092,     527,      87,   11605,     237,    1864,     692,\n",
       "          8063,     173,     488])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 超出前100k词的向量0代替\n",
    "x_train[ x_train >=num_words ] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([    0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
       "           0,     0,     0,     0,     0,     0,     0,  3772,     0,\n",
       "           0,   692,   150,   373, 50177,  2579,   905,  4123,     0,\n",
       "           0,     0, 95151,   107,  1121,  3903,  4261,  3934,  3424,\n",
       "          75, 51906,   738,  3972,    75,   733,   529,   109,   835,\n",
       "        1069,   422,  4663,   158,  2526,   127,   900,     0,   127,\n",
       "         372,   294, 16588,  2232,  1595,  1615, 95151,   201,  2668,\n",
       "        1121, 93097, 18654,   407,  1286,   112])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, GRU, Embedding, LSTM, Bidirectional, Dropout, Flatten\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.python.keras.optimizers import RMSprop\n",
    "from tensorflow.python.keras.optimizers import Adam\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model=Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.add(Embedding(num_words,\n",
    "#                     300,\n",
    "#                     weights=[embedding_matrix],\n",
    "#                     input_length=150,\n",
    "#                     trainable=False))\n",
    "# model.add(Bidirectional(LSTM(units=32, return_sequences=True)))\n",
    "# model.add(LSTM(units=16, return_sequences=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.add(Dense(1, activation='sigmoid'))\n",
    "# 我们使用adam以0.001的learning rate进行优化\n",
    "# optimizer = Adam(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer='rmsprop',\n",
    "#               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "#第一层为嵌入层，用之前的embedding_matrix进行初始化，并设置为不可训练\n",
    "model.add(Embedding(num_words,\n",
    "                    300,\n",
    "                    weights=[embedding_matrix],\n",
    "                    input_length=150,\n",
    "                    trainable=False))\n",
    "#第二层\n",
    "model.add(Dense(128, input_dim=150 , activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "#model.add(Flatten())\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Flatten())\n",
    "#输出层\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='rmsprop',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_12 (Embedding)     (None, 150, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (None, 150, 128)          38528     \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 150, 128)          0         \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (None, 150, 64)           8256      \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 150, 64)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 9600)              0         \n",
      "_________________________________________________________________\n",
      "dense_20 (Dense)             (None, 1)                 9601      \n",
      "=================================================================\n",
      "Total params: 30,056,385\n",
      "Trainable params: 56,385\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_train,\n",
    "                                                    y_train,\n",
    "                                                    test_size=0.1,\n",
    "                                                    random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "22500/22500 [==============================] - 4s 191us/step - loss: 0.2093 - acc: 0.9176\n",
      "Epoch 2/50\n",
      "22500/22500 [==============================] - 4s 179us/step - loss: 0.2055 - acc: 0.9191\n",
      "Epoch 3/50\n",
      "22500/22500 [==============================] - 4s 181us/step - loss: 0.2075 - acc: 0.9178\n",
      "Epoch 4/50\n",
      "22500/22500 [==============================] - 4s 180us/step - loss: 0.2027 - acc: 0.9199\n",
      "Epoch 5/50\n",
      "22500/22500 [==============================] - 4s 182us/step - loss: 0.2004 - acc: 0.9212\n",
      "Epoch 6/50\n",
      "22500/22500 [==============================] - 4s 182us/step - loss: 0.1994 - acc: 0.9223\n",
      "Epoch 7/50\n",
      "22500/22500 [==============================] - 4s 182us/step - loss: 0.1972 - acc: 0.9219\n",
      "Epoch 8/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1908 - acc: 0.9246\n",
      "Epoch 9/50\n",
      "22500/22500 [==============================] - 4s 180us/step - loss: 0.1899 - acc: 0.9260\n",
      "Epoch 10/50\n",
      "22500/22500 [==============================] - 4s 180us/step - loss: 0.1915 - acc: 0.9258\n",
      "Epoch 11/50\n",
      "22500/22500 [==============================] - 4s 181us/step - loss: 0.1889 - acc: 0.9260\n",
      "Epoch 12/50\n",
      "22500/22500 [==============================] - 4s 182us/step - loss: 0.1865 - acc: 0.9276\n",
      "Epoch 13/50\n",
      "22500/22500 [==============================] - 4s 181us/step - loss: 0.1880 - acc: 0.9264\n",
      "Epoch 14/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1853 - acc: 0.9277\n",
      "Epoch 15/50\n",
      "22500/22500 [==============================] - 4s 181us/step - loss: 0.1824 - acc: 0.9285\n",
      "Epoch 16/50\n",
      "22500/22500 [==============================] - 4s 182us/step - loss: 0.1792 - acc: 0.9298\n",
      "Epoch 17/50\n",
      "22500/22500 [==============================] - 4s 181us/step - loss: 0.1798 - acc: 0.9304\n",
      "Epoch 18/50\n",
      "22500/22500 [==============================] - 4s 181us/step - loss: 0.1780 - acc: 0.9306\n",
      "Epoch 19/50\n",
      "22500/22500 [==============================] - 4s 185us/step - loss: 0.1785 - acc: 0.9300\n",
      "Epoch 20/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1743 - acc: 0.9332\n",
      "Epoch 21/50\n",
      "22500/22500 [==============================] - 4s 182us/step - loss: 0.1762 - acc: 0.9349\n",
      "Epoch 22/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1783 - acc: 0.9317\n",
      "Epoch 23/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1754 - acc: 0.9321\n",
      "Epoch 24/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1794 - acc: 0.9324\n",
      "Epoch 25/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1731 - acc: 0.9329\n",
      "Epoch 26/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1667 - acc: 0.9346\n",
      "Epoch 27/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1732 - acc: 0.9330\n",
      "Epoch 28/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1729 - acc: 0.9328\n",
      "Epoch 29/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1661 - acc: 0.9353\n",
      "Epoch 30/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1706 - acc: 0.9348\n",
      "Epoch 31/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1744 - acc: 0.9331\n",
      "Epoch 32/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1649 - acc: 0.9378\n",
      "Epoch 33/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1711 - acc: 0.9349\n",
      "Epoch 34/50\n",
      "22500/22500 [==============================] - 4s 185us/step - loss: 0.1697 - acc: 0.9356\n",
      "Epoch 35/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1672 - acc: 0.9373\n",
      "Epoch 36/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1646 - acc: 0.9378\n",
      "Epoch 37/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1610 - acc: 0.9392\n",
      "Epoch 38/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1641 - acc: 0.9396\n",
      "Epoch 39/50\n",
      "22500/22500 [==============================] - 4s 185us/step - loss: 0.1640 - acc: 0.9380\n",
      "Epoch 40/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1664 - acc: 0.9360\n",
      "Epoch 41/50\n",
      "22500/22500 [==============================] - 4s 186us/step - loss: 0.1627 - acc: 0.9376\n",
      "Epoch 42/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1581 - acc: 0.9407\n",
      "Epoch 43/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1584 - acc: 0.9416\n",
      "Epoch 44/50\n",
      "22500/22500 [==============================] - 4s 185us/step - loss: 0.1601 - acc: 0.9408\n",
      "Epoch 45/50\n",
      "22500/22500 [==============================] - 4s 183us/step - loss: 0.1616 - acc: 0.9386\n",
      "Epoch 46/50\n",
      "22500/22500 [==============================] - 4s 191us/step - loss: 0.1642 - acc: 0.9395\n",
      "Epoch 47/50\n",
      "22500/22500 [==============================] - 4s 189us/step - loss: 0.1552 - acc: 0.9401\n",
      "Epoch 48/50\n",
      "22500/22500 [==============================] - 4s 187us/step - loss: 0.1572 - acc: 0.9416\n",
      "Epoch 49/50\n",
      "22500/22500 [==============================] - 4s 198us/step - loss: 0.1562 - acc: 0.9420\n",
      "Epoch 50/50\n",
      "22500/22500 [==============================] - 4s 184us/step - loss: 0.1626 - acc: 0.9382\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1fa288b4c50>"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          epochs=50,\n",
    "          batch_size=32)\n",
    "#score = model.evaluate(x_test, y_test, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2500/2500 [==============================] - 0s 102us/step\n",
      "Accuracy:86.60%\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate(X_test, y_test)\n",
    "print('Accuracy:{0:.2%}'.format(result[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "m = np.array(y_train) #转换为array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000,)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 3750000 into shape (25000,45000)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-67-26626c402fc6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mxt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m25000\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;36m300\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 3750000 into shape (25000,45000)"
     ]
    }
   ],
   "source": [
    "xt = x_train.reshape(25000,150*300)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 150)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
